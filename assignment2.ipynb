{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "### Olga Iarygina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calssification with the nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment I make a classification with the nearest neighbor method. To select the appropriate model I then proceed to cross-validation and standart data normalization for the aim of preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# read in the data\n",
    "dataTrain = np.loadtxt('OccupancyTrain.csv', delimiter = ',')\n",
    "dataTest = np.loadtxt('OccupancyTest.csv', delimiter = ',')\n",
    "# split input variables and labels\n",
    "XTrain = dataTrain[:, :-1] \n",
    "YTrain = dataTrain[:, -1] \n",
    "XTest = dataTest[:, :-1] \n",
    "YTest = dataTest[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex1 :: nearst neighbor classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all I just implement the knn-classifier with sklearn, specifying the parameter K as 1.\n",
    "Actually we see, that our model is overfitted and predict the labels too well. We should also try different parameters for the model, since at this stage it is not clear whether this number of K is a good choice or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 1)\n",
    "knn.fit(XTrain, YTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the train set: 1.0 \n",
      "Accuracy on the test set: 0.9775\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_train = accuracy_score(YTrain, knn.predict(XTrain))\n",
    "acc_test = accuracy_score(YTest, knn.predict(XTest))\n",
    "print(\"Accuracy on the train set:\", acc_train, \n",
    "      \"\\nAccuracy on the test set:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I decided to try to implemet kNN classifier by my own.\n",
    "I have searched for some solutions on StackOverflow and was inspired to make a class to make it work similar with a sklearn implementation.\n",
    "\n",
    "Well, to build the algorithm let's first remember how does it works. First of all, we should calculate the Euclidean distance, which calculates the square root of the sum of the squared differences between two vectors.\n",
    "\n",
    "Then we proceed to getting nearest neighbors.\n",
    "\n",
    "The principle behind is that for a piece of data the neighbours are the k closest instances, where closure is determined by our Euclidean distance. Euclidean distance is just a length of the line between two points in space.\n",
    "\n",
    "After the calculation the distances between each record in the dataset to a new piece of data, we sort all the records in the training dataset according to their distance to the new data, and select the k most similar neighbors.\n",
    "\n",
    "After we just predict based on the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class knn:\n",
    "\n",
    "    def __init__(self, k = 5):  # I chose this default number as 1 neighbor was returned in sklearn by default\n",
    "        self.k = k\n",
    "\n",
    "    def euclidean_distance(self, a, b): # euclidean distance calculation\n",
    "        dist = 0.0 \n",
    "        \n",
    "        for i in range(len(a)):\n",
    "            dist += (a[i] - b[i]) ** 2\n",
    "            euclidian_distance = np.sqrt(dist)\n",
    "            \n",
    "        return euclidian_distance\n",
    "\n",
    "\n",
    "    def fit(self, XTrain, YTrain): # fitting the model\n",
    "        self.XTrain = XTrain\n",
    "        self.YTrain = YTrain\n",
    "\n",
    "    def predict(self, XTest): # prediction based on getting the nearest neighbors\n",
    "        pred = []\n",
    "\n",
    "        for i in range(len(XTest)):  # here we iterate over the test set\n",
    "            distances = []\n",
    "            for point in self.XTrain:  # for each point in the train set we find a distance to each point in the test set\n",
    "                point_dist = self.euclidean_distance(point, XTest[i])\n",
    "                distances.append(point_dist)\n",
    "\n",
    "            final = np.array(distances).argsort()[: self.k] # here I sort them and remain only specified number of k\n",
    "            neighbors_list = {} # and then we count labels in YTrain\n",
    "            \n",
    "            for m in final:\n",
    "                if self.YTrain[m] in neighbors_list:\n",
    "                    neighbors_list[self.YTrain[m]] += 1\n",
    "                else:\n",
    "                    neighbors_list[self.YTrain[m]] = 1\n",
    "\n",
    "            pred.append(max(neighbors_list, key = neighbors_list.get))\n",
    "            \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the train set: 1.0 \n",
      "Accuracy on the test set: 0.9775\n"
     ]
    }
   ],
   "source": [
    "my_knn = knn(k = 1)\n",
    "my_knn.fit(XTrain, YTrain)\n",
    "\n",
    "acc_train = accuracy_score(YTrain, my_knn.predict(XTrain))\n",
    "acc_test = accuracy_score(YTest, my_knn.predict(XTest))\n",
    "print(\"Accuracy on the train set:\", acc_train, \n",
    "      \"\\nAccuracy on the test set:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it returns me the same accuracies as sklearn (after a thousand of tryings, to be honest). Now we can proceed to the following tasks.\n",
    "\n",
    "My own implementation works much slower than KNeighborClassifier, therefore, I decided to use sklearn version further. Nevertheless, at different stages I checked the work of my model, and the accuracies were the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex2 :: cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the accuracy of the kNN model is determined by the k parameter selection, we need to somehow asses which value of k gives us the best results. To perform that I implement the cross-validation method. The principle behind is that the algorithm splits the dataset into a given number of folds N, and then each of the folds is used once to validate the models. The remaining folds are correspondingly N-1 left.\n",
    "\n",
    "In this task I compared the performance of the model with 1, 3, 5, 7, 9 and 11 neighbors. I estimate the results with a 5-fold CV. After the camparison of different k parameter, I select the model with the lowest average loss and use it for further exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv = KFold(n_splits = 5, random_state = 42, shuffle = True) # initiate 5-fold splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given k = 1 the mean accuracy = 0.9800000000000001\n",
      "Given k = 2 the mean accuracy = 0.9783333333333333\n",
      "Given k = 3 the mean accuracy = 0.9916666666666666\n",
      "Given k = 5 the mean accuracy = 0.9916666666666666\n",
      "Given k = 7 the mean accuracy = 0.9916666666666666\n",
      "Given k = 9 the mean accuracy = 0.9916666666666666\n",
      "Given k = 11 the mean accuracy = 0.99\n"
     ]
    }
   ],
   "source": [
    "num_neighbors = [1, 2, 3, 5, 7, 9, 11] # identify differet values of k parameter to put them into for loop\n",
    "num_accuracy = [] # here I will store the accuracies\n",
    "\n",
    "for i in num_neighbors:\n",
    "    accTest = []\n",
    "    \n",
    "    for train, test in cv.split(XTrain):\n",
    "        XTrainCV, XTestCV, YTrainCV, YTestCV = XTrain[train], XTrain[test], YTrain[train], YTrain[test] # CV splitting\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = i) #fit the model\n",
    "        knn.fit(XTrainCV, YTrainCV)\n",
    "        accuracy = accuracy_score(YTestCV, knn.predict(XTestCV)) # get accuracy\n",
    "        accTest.append(accuracy)\n",
    "        \n",
    "    num_accTest = np.mean(accTest) # find the mean between folds\n",
    "    num_accuracy.append(num_accTest) # store the results\n",
    "    \n",
    "    print(\"Given k =\", i, \"the mean accuracy =\", np.mean(accTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy = 0.9916666666666666\n",
      "The best value of K:  3\n"
     ]
    }
   ],
   "source": [
    "print(\"The best accuracy =\", np.max(num_accuracy))\n",
    "print(\"The best value of K: \", num_neighbors[np.argmax(num_accuracy)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we can see, that the best model is when the number of K = 3. Noticeable that we have the same accuracies for the models with a higher number of K. But there is no need to add extra number of neighbors, since we already have a good result with K = 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex3 :: evaluation of classification performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task I simply evaluate the model with the number of K I revealed is the best by applying cross-validation.\n",
    "\n",
    "Actually, we can see that even though the sample is splitted tinto test and train sets, the model seems to be overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the train set: 0.9933333333333333 \n",
      "Accuracy on the test set: 0.9875\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(XTrain, YTrain)\n",
    "\n",
    "acc_train = accuracy_score(YTrain, knn.predict(XTrain))\n",
    "acc_test = accuracy_score(YTest, knn.predict(XTest))\n",
    "print(\"Accuracy on the train set:\", acc_train, \n",
    "      \"\\nAccuracy on the test set:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex4 :: data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task I normalize the data. TWe use the standart normalization technique here, which is to generate zero-mean, unit variance input data. Firtly, I use scikit-learn implementation.\n",
    "\n",
    "The appropriate solution from the suggeted in the text of the assignment is the first one. We cannot use the second one, because we are basing ourselves on the train data. The third one is inappropriate, because we are not supposed to use future data in normalization. They should be independent, otherwise testing will not make much sence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(XTrain) \n",
    "XTrainN = scaler.transform(XTrain) \n",
    "XTestN = scaler.transform(XTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given k = 1 the mean accuracy = 0.9833333333333332\n",
      "Given k = 2 the mean accuracy = 0.9850000000000001\n",
      "Given k = 3 the mean accuracy = 0.9916666666666666\n",
      "Given k = 5 the mean accuracy = 0.99\n",
      "Given k = 7 the mean accuracy = 0.99\n",
      "Given k = 9 the mean accuracy = 0.99\n",
      "Given k = 11 the mean accuracy = 0.99\n"
     ]
    }
   ],
   "source": [
    "num_neighbors = [1, 2, 3, 5, 7, 9, 11] # identify differet values of k parameter to put them into for loop\n",
    "num_accuracy = [] # here I will store the accuracies\n",
    "\n",
    "for i in num_neighbors:\n",
    "    accTest = []\n",
    "    \n",
    "    for train, test in cv.split(XTrain):\n",
    "        XTrainCV, XTestCV, YTrainCV, YTestCV = XTrainN[train], XTrainN[test], YTrain[train], YTrain[test] # CV splitting\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = i) #fit the model\n",
    "        knn.fit(XTrainCV, YTrainCV)\n",
    "        accuracy = accuracy_score(YTestCV, knn.predict(XTestCV)) # get accuracy\n",
    "        accTest.append(accuracy)\n",
    "        \n",
    "    num_accTest = np.mean(accTest) # find the mean between folds\n",
    "    num_accuracy.append(num_accTest) # store the results\n",
    "    \n",
    "    print(\"Given k =\", i, \"the mean accuracy =\", np.mean(accTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy = 0.9916666666666666\n",
      "The best value of K:  3\n"
     ]
    }
   ],
   "source": [
    "print(\"The best accuracy =\", np.max(num_accuracy))\n",
    "print(\"The best value of K: \", num_neighbors[np.argmax(num_accuracy)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the comparisons of different K, we have slightly different results in accuracies. But still the best model is still with the number of K equals 3. And the accuracy of this model on the train and test sets remained the same. I can suggest, that probably our data was already ditributed normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the train set: 0.9933333333333333 \n",
      "Accuracy on the test set: 0.9875\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(XTrainN, YTrain)\n",
    "\n",
    "acc_train = accuracy_score(YTrain, knn.predict(XTrainN))\n",
    "acc_test = accuracy_score(YTest, knn.predict(XTestN))\n",
    "\n",
    "print(\"Accuracy on the train set:\", acc_train, \n",
    "      \"\\nAccuracy on the test set:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I decidet to try to perform normalization by my on, following the procedure described in the text of the assignment. So, we are first finding variance, mean and standard deviation, And them from each point we subtract the mean score, and divide this all by the standard deviation to get the centered results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train statistic values\n",
    "XTrain_mean = np.mean(XTrain, axis = 0)\n",
    "XTrain_std = np.std(XTrain, axis = 0)\n",
    "XTrain_var = np.var(XTrain, axis = 0)\n",
    "\n",
    "# test statistic values\n",
    "XTest_mean = np.mean(XTest, axis = 0)\n",
    "XTest_std = np.std(XTest, axis = 0)\n",
    "XTest_var = np.var(XTest, axis = 0)\n",
    "\n",
    "# normalization\n",
    "XTrainN = (XTrain - XTrain_mean)/XTrain_std\n",
    "XTestN = (XTest - XTrain_mean)/XTrain_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then I just test whether I got the same results with my own normalization.\n",
    "Luckily, everything is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given k = 1 the mean accuracy = 0.9833333333333332\n",
      "Given k = 2 the mean accuracy = 0.9850000000000001\n",
      "Given k = 3 the mean accuracy = 0.9916666666666666\n",
      "Given k = 5 the mean accuracy = 0.99\n",
      "Given k = 7 the mean accuracy = 0.99\n",
      "Given k = 9 the mean accuracy = 0.99\n",
      "Given k = 11 the mean accuracy = 0.99\n"
     ]
    }
   ],
   "source": [
    "num_neighbors = [1, 2, 3, 5, 7, 9, 11] # identify differet values of k parameter to put them into for loop\n",
    "num_accuracy = [] # here I will store the accuracies\n",
    "\n",
    "for i in num_neighbors:\n",
    "    accTest = []\n",
    "    \n",
    "    for train, test in cv.split(XTrain):\n",
    "        XTrainCV, XTestCV, YTrainCV, YTestCV = XTrainN[train], XTrainN[test], YTrain[train], YTrain[test] # CV splitting\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = i) #fit the model\n",
    "        knn.fit(XTrainCV, YTrainCV)\n",
    "        accuracy = accuracy_score(YTestCV, knn.predict(XTestCV)) # get accuracy\n",
    "        accTest.append(accuracy)\n",
    "        \n",
    "    num_accTest = np.mean(accTest) # find the mean between folds\n",
    "    num_accuracy.append(num_accTest) # store the results\n",
    "    \n",
    "    print(\"Given k =\", i, \"the mean accuracy =\", np.mean(accTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the train set: 0.9933333333333333 \n",
      "Accuracy on the test set: 0.9875\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(XTrainN, YTrain)\n",
    "\n",
    "acc_train = accuracy_score(YTrain, knn.predict(XTrainN))\n",
    "acc_test = accuracy_score(YTest, knn.predict(XTestN))\n",
    "\n",
    "print(\"Accuracy on the train set:\", acc_train, \n",
    "      \"\\nAccuracy on the test set:\", acc_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
